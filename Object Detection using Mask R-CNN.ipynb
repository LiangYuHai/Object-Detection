{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Model created in this test task uses Mask R-CNN API from <a href=https://github.com/matterport/Mask_RCNN>here</a> and transfer learning to detect safety cones in given images.\n",
    "\n",
    "Images were extracted from the videos using VLC media player due to issues with OpenCV for Python and labeled using labelImg from <a href=https://github.com/tzutalin/labelImg>here</a>.\n",
    "\n",
    "Annotations created using labelImg were saved to an annotations folder in this notebooks directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Dataset preparation\n",
    "All folders and libraries were set up before running code in this notebook. \n",
    "\n",
    "Required libraries for this notebook are available in <a href=https://github.com/witcher346/Object-Detection>this</a> repository, in requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.1 - Import libraries and split images to train/test split\n",
    "### Import libraries for Mask R-CNN API preparation\n",
    "Also Mask R-CNN and COCO weights must be either git cloned from <a href=https://github.com/matterport/Mask_RCNN>here</a> or downloaded and unzipped into this notebooks folder for the code to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir, makedirs\n",
    "from shutil import copy\n",
    "from xml.etree import ElementTree\n",
    "from numpy import zeros\n",
    "from numpy import asarray\n",
    "from random import shuffle\n",
    "from Mask_RCNN.mrcnn.utils import Dataset\n",
    "from Mask_RCNN.mrcnn.utils import extract_bboxes\n",
    "from mrcnn.visualize import display_instances\n",
    "from Mask_RCNN.mrcnn.config import Config\n",
    "from Mask_RCNN.mrcnn.model import MaskRCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method to split the images into train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(images_path='images', \n",
    "                     train_set_dst='images/train', \n",
    "                     test_set_dst='images/test', \n",
    "                     train_set_size=22):\n",
    "    try:\n",
    "        # try to create these directories in case they are not created\n",
    "        makedirs(f'{images_path}/train') \n",
    "        makedirs(f'{images_path}/test')\n",
    "        \n",
    "    except FileExistsError:\n",
    "        print('Directories already exist. Trying to split images.')\n",
    "        \n",
    "    finally:\n",
    "        # only include images from images folder\n",
    "        images = [img for img in listdir(images_path) if '.jpg' in img] \n",
    "        # randomly shuffle the images\n",
    "        shuffle(images) \n",
    "        \n",
    "        # create train/test sets from random shuffle\n",
    "        train_set = images[:train_set_size] \n",
    "        test_set = images[train_set_size:]\n",
    "        \n",
    "        # distribute the images\n",
    "        for image in train_set:\n",
    "            copy(f'{images_path}\\\\{image}', train_set_dst) \n",
    "        for annotation in test_set:\n",
    "            copy(f'{images_path}\\\\{image}', test_set_dst)\n",
    "        \n",
    "    print('Split successful.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.2 - Dataset class interpretation\n",
    "### Use inheritance to redefine Dataset class from Mask R-CNN API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class that defines and loads the safety cones dataset\n",
    "class SafetyCones(Dataset):\n",
    "    ROOT = 'C:/Users/User/Jupyter Notebooks/Object Detection/'\n",
    "    \n",
    "    # load the dataset definitions\n",
    "    def load_dataset(self, is_train=True):\n",
    "        # create train/test split\n",
    "        train_test_split()\n",
    "        # define one class\n",
    "        self.add_class(\"dataset\", 1, \"safety_cone\")\n",
    "        # define data locations depending on is_train flag\n",
    "        images_dir = 'images/'\n",
    "        if is_train:\n",
    "            images_dir += 'train/'\n",
    "        else:\n",
    "            images_dir += 'test/'\n",
    "            \n",
    "        annotations_dir = 'annotations/'\n",
    "        # find all images\n",
    "        for filename in listdir(images_dir):\n",
    "            # extract image id\n",
    "            if not filename.endswith('.xml'):\n",
    "                image_id = filename[:-4]\n",
    "                img_path = images_dir + filename\n",
    "                # create annotation path\n",
    "                ann_path = annotations_dir + image_id + '.xml'\n",
    "                # add to dataset\n",
    "                self.add_image('dataset', image_id=image_id, \n",
    "                               path=self.ROOT+img_path, \n",
    "                               annotation=self.ROOT+ann_path)\n",
    " \n",
    "    # extract bounding boxes from an annotation file\n",
    "    def extract_boxes(self, filename):\n",
    "        # load and parse the file\n",
    "        tree = ElementTree.parse(filename)\n",
    "        # get the root of the document\n",
    "        root = tree.getroot()\n",
    "        # extract each bounding box coordinates on an image\n",
    "        boxes = []\n",
    "        for box in root.findall('.//bndbox'):\n",
    "            xmin = int(box.find('xmin').text)\n",
    "            ymin = int(box.find('ymin').text)\n",
    "            xmax = int(box.find('xmax').text)\n",
    "            ymax = int(box.find('ymax').text)\n",
    "            coors = [xmin, ymin, xmax, ymax]\n",
    "            boxes.append(coors)\n",
    "        # extract image dimensions\n",
    "        width = int(root.find('.//size/width').text)\n",
    "        height = int(root.find('.//size/height').text)\n",
    "        return boxes, width, height\n",
    " \n",
    "    # load the masks for an image\n",
    "    def load_mask(self, image_id):\n",
    "        # get details of image\n",
    "        info = self.image_info[image_id]\n",
    "        # define box file location\n",
    "        path = info['annotation']\n",
    "        # load XML\n",
    "        boxes, w, h = self.extract_boxes(path)\n",
    "        # create one array for all masks, each on a different channel\n",
    "        masks = zeros([h, w, len(boxes)], dtype='uint8')\n",
    "        # create masks\n",
    "        class_ids = []\n",
    "        for i in range(len(boxes)):\n",
    "            box = boxes[i]\n",
    "            row_s, row_e = box[1], box[3]\n",
    "            col_s, col_e = box[0], box[2]\n",
    "            masks[row_s:row_e, col_s:col_e, i] = 1\n",
    "            class_ids.append(self.class_names.index('safety_cone'))\n",
    "        return masks, asarray(class_ids, dtype='int32')\n",
    " \n",
    "    # load an image reference\n",
    "    def image_reference(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        return info['path']\n",
    "\n",
    "# define a configuration for the model\n",
    "class SafetyConeConfig(Config):\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"cone_cfg\"\n",
    "    # Number of classes (background + cone)\n",
    "    NUM_CLASSES = 1 + 1\n",
    "    # Number of training steps per epoch\n",
    "    STEPS_PER_EPOCH = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = SafetyCones()\n",
    "train_set.load_dataset(is_train=True)\n",
    "train_set.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = SafetyCones()\n",
    "test_set.load_dataset(is_train=False)\n",
    "test_set.prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the image with masks and boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define image id\n",
    "image_id = 1\n",
    "# load the image\n",
    "image = train_set.load_image(image_id)\n",
    "# load the masks and the class ids\n",
    "mask, class_ids = train_set.load_mask(image_id)\n",
    "# extract bounding boxes from the masks\n",
    "bbox = extract_bboxes(mask)\n",
    "# display image with masks and bounding boxes\n",
    "display_instances(image, bbox, mask, class_ids, train_set.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model for 3 epochs 100 steps each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare config\n",
    "config = SafetyConeConfig()\n",
    "# define the model\n",
    "model = MaskRCNN(mode='training', model_dir='./', config=config)\n",
    "# load weights and exclude the output layers\n",
    "model.load_weights('mask_rcnn_coco.h5', by_name=True, \n",
    "                   exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\",  \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "# train weights (output layers or 'heads')\n",
    "model.train(train_set, test_set, learning_rate=config.LEARNING_RATE, epochs=3, layers='heads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Evaluating the model\n",
    "For model evaluation mean average precision (mAP) will be used. It shows the average precision (AP) of a model across all of the images in a dataset. \n",
    "\n",
    "The mask-rcnn library provides a mrcnn.utils.compute_ap to calculate the AP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import expand_dims\n",
    "from numpy import mean\n",
    "from Mask_RCNN.mrcnn.utils import compute_ap\n",
    "from Mask_RCNN.mrcnn.model import load_image_gt\n",
    "from Mask_RCNN.mrcnn.model import mold_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mAP for a model on a given dataset\n",
    "def evaluate_model(dataset, model, cfg):\n",
    "    APs = []\n",
    "    print('Starting evaluation...')\n",
    "    for image_id in dataset.image_ids:\n",
    "        # load image, bounding boxes and masks for the image id\n",
    "        image, image_meta, gt_class_id, gt_bbox, gt_mask = load_image_gt(dataset, cfg, image_id, use_mini_mask=False)\n",
    "        # convert pixel values (e.g. center)\n",
    "        scaled_image = mold_image(image, cfg)\n",
    "        # convert image into one sample\n",
    "        sample = expand_dims(scaled_image, 0)\n",
    "        # make prediction\n",
    "        yhat = model.detect(sample, verbose=0)\n",
    "        # extract results for first sample\n",
    "        r = yhat[0]\n",
    "        # calculate statistics, including AP\n",
    "        AP, _, _, _ = compute_ap(gt_bbox, gt_class_id, gt_mask, r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "        # store\n",
    "        APs.append(AP)\n",
    "    # calculate the mean AP across all images\n",
    "    mAP = mean(APs)\n",
    "    return mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config is changed so that model could predict each image individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplify GPU config\n",
    "config.GPU_COUNT = 1\n",
    "config.IMAGES_PER_GPU = 1\n",
    "config.BATCH_SIZE = 1\n",
    "# define the model\n",
    "model = MaskRCNN(mode='inference', model_dir='./', config=config)\n",
    "# load trained model weights\n",
    "model.load_weights('cone_cfg20200710T1702/mask_rcnn_cone_cfg_0003.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The code snippet below should return the following results**\n",
    "- Train mAP: 0.943\n",
    "- Test mAP: 0.970"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mAP = evaluate_model(train_set, model, config)\n",
    "print(\"Train mAP: %.3f\" % train_mAP)\n",
    "# evaluate model on test dataset\n",
    "test_mAP = evaluate_model(test_set, model, config)\n",
    "print(\"Test mAP: %.3f\" % test_mAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Predicting new images\n",
    "Each image in test-imgs folder represents 1 frame of a 50 seconds clip provided for demonstration purposes\n",
    "\n",
    "Here new images are loaded, predicted and saved to predictions folder to be compiled into a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(_dir, model, cfg):\n",
    "    # load images\n",
    "    ROOT = 'C:/Users/User/Jupyter Notebooks/Object Detection/'\n",
    "    for img in os.listdir(ROOT+_dir):\n",
    "        # load the image via skimage and PIL\n",
    "        time_spent = time.time()\n",
    "        path_to_img = ROOT+_dir+img\n",
    "        image = skimage.io.imread(path_to_img)\n",
    "        # open the same image in PIL to draw a boundry box if the model predicts one\n",
    "        tmp_image = Image.open(path_to_img)\n",
    "        # convert pixel values (e.g. center)\n",
    "        scaled_image = mold_image(image, cfg)\n",
    "        # convert image into one sample\n",
    "        sample = expand_dims(scaled_image, 0)\n",
    "        # make prediction\n",
    "        yhat = model.detect(sample, verbose=0)[0]\n",
    "        # draw a rectangle\n",
    "        draw = ImageDraw.Draw(tmp_image)\n",
    "        for box in yhat['rois']:\n",
    "            y1, x1, y2, x2 = box\n",
    "            draw.rectangle(((x1, y1), (x2, y2)), outline='red')\n",
    "        # save the image with box\n",
    "        tmp_image.save(ROOT+'predictions/'+img)\n",
    "        print(f'Predicted {img} in {round(time.time() - time_spent)}s and saved it to predictions folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction('test-imgs/', model, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "After running this notebook there should be a predictions folder full of images with or without bounding boxes around the safety cones. Images in prediction folder are compiled into a video using Adobe After Effects\n",
    "\n",
    "**Next steps in improving models performence can be:**\n",
    "- Include test set in the training after making sure that the model yields good mAP results and does not overfit;\n",
    "- Use more images;\n",
    "- Use images of a better resolution;\n",
    "- Remove the text snippets in all of the images;\n",
    "- Try to move to Tensorflow's Object Detection API and see if those models yield better results;\n",
    "- Only train on images that include atleast one bounding box;\n",
    "- Create batches of different images for training, where there are only 1 cone for image or 1 and 2 cones, or only 2 cones and so on.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
